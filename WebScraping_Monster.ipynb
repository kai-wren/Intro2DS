{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "import re\n",
    "from urllib.request import urlopen as uReq\n",
    "from urllib.request import Request as Req\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWord = 'Data-Science'\n",
    "urlorig='https://www.monster.com/jobs/search/?q='+keyWord+'&where=USA'\n",
    "urlorig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "company_names = []\n",
    "location_names = []\n",
    "job_descriptions = []\n",
    "job_ids = []\n",
    "refs=[]\n",
    "\n",
    "global_ind=0\n",
    "\n",
    "# defining parser to call for each new page\n",
    "def monster_parser(url):\n",
    "    global global_ind\n",
    "    request = Req(url)\n",
    "    request.add_header('Cache-Control', 'no-cache')\n",
    "    uClient = uReq(request)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,'html.parser')\n",
    "    jobs = page_soup.select('section.card-content')\n",
    "    \n",
    "    local_ind=0 #change to 1 and check result\n",
    "    for job in jobs:\n",
    "        if job.get('data-jobid')!=None:\n",
    "            if local_ind < global_ind:\n",
    "                local_ind+=1\n",
    "            else:\n",
    "                local_ind+=1\n",
    "                global_ind+=1\n",
    "                summaries = job.div.find_all('div', class_='summary')\n",
    "                summary=summaries[0]\n",
    "\n",
    "                try:\n",
    "                    #check if vacancy exist\n",
    "                    tmp_client = uReq(summary.header.h2.a['href'])\n",
    "                    tmp_client.close()\n",
    "\n",
    "                    print(global_ind, local_ind, summary.header.h2.a['href'])\n",
    "\n",
    "                    #fetch Job ID\n",
    "                    try:\n",
    "                        job_ids.append(job['data-jobid'])\n",
    "                    except:\n",
    "                        job_ids.append('NA')\n",
    "\n",
    "                    #fetching URLs\n",
    "                    try:\n",
    "                        refs.append(summary.header.h2.a['href'])\n",
    "                    except:\n",
    "                        refs.append('NA')\n",
    "\n",
    "                    #fetch title\n",
    "                    try:\n",
    "                        titles.append(summary.header.h2.a.text.strip())\n",
    "                    except:\n",
    "                        titles.append('NA')\n",
    "                    #fetch company\n",
    "                    try:\n",
    "                        company_names.append(summary.div.span.text)\n",
    "                    except:\n",
    "                        company_names.append('NA')\n",
    "\n",
    "        #             locations = summary.find_all('div', class_='location')\n",
    "        #             location = locations[0]\n",
    "        #             #fetch location\n",
    "        #             try:\n",
    "        #                 location_names.append(location.span.text.strip())\n",
    "        #             except:\n",
    "        #                 location_names.append('NA')\n",
    "                    #parse job description and details\n",
    "                    monster_details_parser(summary.header.h2.a['href'])\n",
    "\n",
    "                except:\n",
    "                     print('SKIP - ', summary.header.h2.a['href'])\n",
    "                    \n",
    "    next_page = page_soup.select('div.mux-search-results a#loadMoreJobs.mux-btn.btn-secondary')\n",
    "    next_url = next_page[0]['href']+'&q=Data-Science&where=USA'\n",
    "    monster_parser(next_url)\n",
    "                \n",
    "def monster_details_parser(url):\n",
    "    uClient = uReq(url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,'html.parser')\n",
    "    \n",
    "    descr = page_soup.select('div#JobDescription')\n",
    "    try:\n",
    "        job_descriptions.append(descr[0].text)\n",
    "    except:\n",
    "        job_descriptions.append('NA')\n",
    "        \n",
    "    values = page_soup.select('div.mux-job-summary section.summary-section dl.header dd.value')\n",
    "    for value in values:\n",
    "        if value.parent.dt.text == \"Location\":\n",
    "            #fetch location\n",
    "            try:\n",
    "                location_names.append(value.text)\n",
    "            except:\n",
    "                location_names.append('NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = urlorig\n",
    "\n",
    "monster_parser(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single page parser testing\n",
    "monster_details_parser('https://job-openings.monster.com/principal-data-scientist-facilities-analytics-redondo-beach-ca-us-northrop-grumman/58367067-6657-41c9-abb6-46a6aefdf6eb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
