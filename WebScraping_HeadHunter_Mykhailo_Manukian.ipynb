{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries needed to fetch web-site content and create data file respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mykhaylo_manukyan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from bs4) (4.8.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\mykhaylo_manukyan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-cp37-none-any.whl size=1278 sha256=6f36459859975fbaf8a68aaae2066cece898383d339e0f75c249a9fc8ee90abe\n",
      "  Stored in directory: C:\\Users\\Mykhaylo_Manukyan\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "import re\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating starting URL pointing to search result page with open vacancies on headhunter.com web site using \"data science\" as search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWord = 'data science'\n",
    "keyWord = keyWord.replace(' ', '%20')\n",
    "urlorig='https://www.headhunter.com/jobs?Keywords='+keyWord+'&Radius=150&PageNumber=1&OrderBy=Relevance'\n",
    "urlorig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code should go through search results and extract links which pointing to full list of jobs per company. This is specific of headhunter.com web site. Since list of jobs per company has also multiple number of pages - code will recursively call method to fetch all next pages till end of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(soup_param, res_list):\n",
    "    pages = soup_param.findAll(\"a\", class_ = \"searchnext\")\n",
    "    if len(pages) > 0:\n",
    "        link = pages[0]\n",
    "        link_esc = link[\"href\"].replace(' ', '%20')\n",
    "        print(link[\"href\"])\n",
    "        Jb_page = uReq(\"https://www.headhunter.com\" + link_esc)\n",
    "        Jb_page_html = Jb_page.read()\n",
    "        Jb_page_soup = soup(Jb_page_html, 'html.parser')\n",
    "        Jb_page.close()\n",
    "        res_list.append(Jb_page_soup)\n",
    "        get_pages(Jb_page_soup, res_list)\n",
    "\n",
    "\n",
    "my_url = urlorig\n",
    "uClient = uReq(my_url)\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html,'html.parser')\n",
    "\n",
    "jb_all = []\n",
    "\n",
    "\n",
    "links_all_jobs = page_soup.findAll(\"a\", class_ = \"see_all_jobs_link\")\n",
    "for link in links_all_jobs:\n",
    "    Jb_all = uReq(\"https://www.headhunter.com\" + link[\"href\"])\n",
    "#     print(link[\"href\"])\n",
    "    Jb_all_html = Jb_all.read()\n",
    "    Jb_all.close()\n",
    "    Jb_all_soup = soup(Jb_all_html, \"html.parser\")\n",
    "    jb_all.append(Jb_all_soup)\n",
    "    \n",
    "all_pages=[]\n",
    "for item in jb_all:\n",
    "    get_pages(item, all_pages)\n",
    "\n",
    "jb_all.extend(all_pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below were used for try and error method of parsing single job details page. I used this to speed up trouble-shooting and finetune code, which will be later executed in cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#single vacancy page\n",
    "# # my_url1 = \"https://www.headhunter.com/jobs/Data-Analytics-Manager/J3M2FR6FRKHG9GZFN1W\"\n",
    "# my_url1 = \"https://www.headhunter.com/jobs/Senior-Data-Scientist/J3N61M605D08SG4PV0C\"\n",
    "# uClient1 = uReq(my_url1)\n",
    "# page_html1 = uClient1.read()\n",
    "# uClient1.close()\n",
    "# page_soup1 = soup(page_html1,'html.parser')\n",
    "\n",
    "# print(page_soup1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = page_soup1.findAll(\"div\", class_=\"content\")\n",
    "# print(len(content))\n",
    "# for i in content:\n",
    "#     print(i.text)\n",
    "#     refID = re.search(\"Ref ID: [0-9]+-+[0-9]+\", content.text)\n",
    "#     print(refID.group()[8:])\n",
    "\n",
    "# job_text = page_soup1.findAll(\"div\", id=(\"description-container-premium\", \"description-container-standard\"))\n",
    "# item = job_text[0]\n",
    "# contents = item.select(\"div.content\")\n",
    "# # print(contents)\n",
    "\n",
    "# refID = re.search(\"Ref ID: [0-9]+-+[0-9]+\", contents[0].text)\n",
    "# if refID != None:\n",
    "#     print(refID.group()[8:])\n",
    "\n",
    "# descr=''\n",
    "# for content in contents:\n",
    "\n",
    "    \n",
    "#     descr+= content.text\n",
    "    \n",
    "#     for i in content.select(\"div#job-snapshot-container.content div.jobs-detail-summary-item span\"):\n",
    "#         if i.text == 'Location ':\n",
    "#             print(i.next_sibling.string.next_sibling.text)\n",
    "#         if i.text == 'Employment Type ':\n",
    "#             print(i.next_sibling.string.next_sibling.text)\n",
    "#         if i.text == 'Pay Type ':\n",
    "#             print(i.next_sibling.string.next_sibling.text)\n",
    "#         if i.text == 'Pay Rate ':\n",
    "#             print(i.next_sibling.string.next_sibling.text)\n",
    "#         if i.text == 'Store Type ':\n",
    "#             print(i.next_sibling.string.next_sibling.text)\n",
    "#         if i.text == 'Other Compensation: ':\n",
    "#             print(i.next_sibling.string.next_sibling.text)\n",
    "# # print(descr)\n",
    "\n",
    "# if(\"statistic\" in descr.lower()):\n",
    "#     print('1')\n",
    "# else:\n",
    "#     print('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is a parser itself for headhunter.com web site. Search result itself are presented as short list (3 jobs per company) with a link leads to all jobs per company. Hence parsing happens in two steps - first parse all jobs from short list. Next step, parse all details from jobs under relative web pages (including sub-pages of each full job list per company). Code requires refactoring to exclude copy-paste, but currently it works and fetch all jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "company_names = []\n",
    "location_names = []\n",
    "compensation = []\n",
    "job_descriptions = []\n",
    "job_ids = []\n",
    "employment_types = []\n",
    "pay_types=[]\n",
    "pay_rates=[]\n",
    "store_types=[]\n",
    "other_compensations=[]\n",
    "refs=[]\n",
    "skills = {}\n",
    "skills['Python'] = []\n",
    "skills['R '] = []\n",
    "skills['Matlab'] = []\n",
    "skills['Excel'] = []\n",
    "skills['SAS'] = []\n",
    "skills['SQL'] = []\n",
    "skills['SPSS'] = []\n",
    "skills['Hadoop'] = []\n",
    "skills['Spark'] = []\n",
    "skills['Algebra'] = []\n",
    "skills['Statistic'] = []\n",
    "skills['Mathematics'] = []\n",
    "skills['Reinforcement_Learning'] = []\n",
    "skills['Deep_Learning'] = []\n",
    "skills['Machine_Learning'] = []\n",
    "skills['Data_Visualization'] = []\n",
    "skills['Keras'] = []\n",
    "skills['TensorFlow'] = []\n",
    "skills['Artificial_Intelligence'] = []\n",
    "skills['Communications'] = []\n",
    "skills['Team_Player'] = []\n",
    "skills['Empathy'] = []\n",
    "skills['Emotional_Intellect'] = []\n",
    "skills['Self_Improvement'] = []\n",
    "skills['Presentation'] = []\n",
    "skills['Critical_Thinking'] = []\n",
    "skills['Business_Understanding'] = []\n",
    "\n",
    "# parsing short list of vacancies\n",
    "\n",
    "# extract job_title\n",
    "jobs = page_soup.findAll(\"div\", class_=re.compile(\"searchresult clearfix\"))\n",
    "for job in jobs:\n",
    "    try:\n",
    "        titles.append(job[\"title\"])\n",
    "    except:\n",
    "        titles.append(\"NA\")\n",
    "\n",
    "# print(titles)\n",
    "\n",
    "# extract company_name\n",
    "companies = page_soup.findAll(\"span\", class_=\"resultcompany_mobile clearfix\" )\n",
    "for company in companies:\n",
    "    try:\n",
    "        company_names.append(company.text.strip())\n",
    "    except:\n",
    "        company_names.append(\"NA\")\n",
    "        \n",
    "# print(company_names)\n",
    "\n",
    "# extract location       \n",
    "locations = page_soup.findAll(\"span\", class_=\"result-right-one\")\n",
    "for location in locations:\n",
    "    try:\n",
    "        location_names.append(location.text)\n",
    "    except:\n",
    "        location_names.append(\"NA\")\n",
    "        \n",
    "# print(location_names)\n",
    "\n",
    "# extract compensation       \n",
    "payments = page_soup.findAll(\"span\", class_=\"result-right-two non-mobile\")\n",
    "for payment in payments:\n",
    "    try:\n",
    "        compensation.append(payment.text)\n",
    "    except:\n",
    "        compensation.append(\"NA\")\n",
    "        \n",
    "# print(compensation)\n",
    "\n",
    "#extract job_id and full description\n",
    "links = page_soup.findAll(\"a\", class_ = \"result-left\")\n",
    "ind=0\n",
    "for link in links:\n",
    "    ind+=1\n",
    "    print(ind, link[\"href\"])\n",
    "    Jb = uReq(\"https://www.headhunter.com\" + link[\"href\"])\n",
    "    Jb_html = Jb.read()\n",
    "    Jb.close()\n",
    "    Jb_soup = soup(Jb_html, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        refs.append(\"https://www.headhunter.com\" + link[\"href\"])\n",
    "    except:\n",
    "        refs.append(\"NA\")\n",
    "    \n",
    "    job_text = Jb_soup.findAll(\"div\", id=(\"description-container-premium\", \"description-container-standard\"))\n",
    "    item = job_text[0]\n",
    "    contents = item.select(\"div.content\")\n",
    "\n",
    "    refID = re.search(\"Ref ID: [0-9]+-+[0-9]+\", contents[0].text)\n",
    "    try:\n",
    "        job_ids.append(refID.group()[8:])\n",
    "    except:\n",
    "        job_ids.append(\"NA\")\n",
    "    \n",
    "    descr=''\n",
    "    other_comp_flag=False\n",
    "    for content in contents:\n",
    "        \n",
    "        descr+= content.text\n",
    "        \n",
    "    \n",
    "    \n",
    "        for i in content.select(\"div#job-snapshot-container.content div.jobs-detail-summary-item span\"):\n",
    "    #         if i.text == 'Location ':\n",
    "    #             print(i.next_sibling.string.next_sibling.text)\n",
    "\n",
    "    #         other_compensations.append(\"NA\")\n",
    "\n",
    "            if i.text == 'Employment Type ':\n",
    "                try:\n",
    "                    employment_types.append(i.next_sibling.string.next_sibling.text)\n",
    "                except:\n",
    "                    employment_types.append(\"NA\")\n",
    "            if i.text == 'Pay Type ':\n",
    "                try:\n",
    "                    pay_types.append(i.next_sibling.string.next_sibling.text)\n",
    "                except:\n",
    "                    pay_types.append(\"NA\")\n",
    "            if i.text == 'Pay Rate ':\n",
    "                try:\n",
    "                    pay_rates.append(i.next_sibling.string.next_sibling.text)\n",
    "                except:\n",
    "                    pay_rates.append(\"NA\")\n",
    "            if i.text == 'Store Type ':\n",
    "                try:\n",
    "                    store_types.append(i.next_sibling.string.next_sibling.text)\n",
    "                except:\n",
    "                    store_types.append(\"NA\")\n",
    "            if i.text == 'Other Compensation: ':\n",
    "                other_comp_flag=True\n",
    "                try:\n",
    "                    other_compensations.append(i.next_sibling.string.next_sibling.text)\n",
    "                except:\n",
    "                    other_compensations.append(\"NA\")\n",
    "                \n",
    "    try:\n",
    "        job_descriptions.append(str(re.sub('(\\n|\\r)', '',descr)))\n",
    "    except:\n",
    "        job_descriptions.append(\"NA\")\n",
    "        \n",
    "# looking for skills - Excell\n",
    "    if(\"excel\" in descr.lower()):\n",
    "        skills['Excel'].append('1')\n",
    "    else:\n",
    "        skills['Excel'].append('0')\n",
    "# looking for skills - Python    \n",
    "    if(\"python\" in descr.lower()):\n",
    "        skills['Python'].append('1')\n",
    "    else:\n",
    "        skills['Python'].append('0')\n",
    "# looking for skills - R\n",
    "    if(\"R \" in descr):\n",
    "        skills['R '].append('1')\n",
    "    else:\n",
    "        skills['R '].append('0')\n",
    "# looking for skills - Matlab\n",
    "    if(\"matlab\" in descr.lower()):\n",
    "        skills['Matlab'].append('1')\n",
    "    else:\n",
    "        skills['Matlab'].append('0')\n",
    "# looking for skills - SAS\n",
    "    if(\"sas\" in descr.lower()):\n",
    "        skills['SAS'].append('1')\n",
    "    else:\n",
    "        skills['SAS'].append('0')\n",
    "# looking for skills - SQL\n",
    "    if(\"sql\" in descr.lower()):\n",
    "        skills['SQL'].append('1')\n",
    "    else:\n",
    "        skills['SQL'].append('0')\n",
    "# looking for skills - SPSS\n",
    "    if(\"spss\" in descr.lower()):\n",
    "        skills['SPSS'].append('1')\n",
    "    else:\n",
    "        skills['SPSS'].append('0')\n",
    "# looking for skills - Hadoop\n",
    "    if(\"hadoop\" in descr.lower()):\n",
    "        skills['Hadoop'].append('1')\n",
    "    else:\n",
    "        skills['Hadoop'].append('0')\n",
    "# looking for skills - Spark\n",
    "    if(\"spark\" in descr.lower()):\n",
    "        skills['Spark'].append('1')\n",
    "    else:\n",
    "        skills['Spark'].append('0')\n",
    "# looking for skills - Algebra\n",
    "    if(\"algebra\" in descr.lower()):\n",
    "        skills['Algebra'].append('1')\n",
    "    else:\n",
    "        skills['Algebra'].append('0')\n",
    "# looking for skills - Statistic\n",
    "    if(\"statistic\" in descr.lower()):\n",
    "        skills['Statistic'].append('1')\n",
    "    else:\n",
    "        skills['Statistic'].append('0')\n",
    "# looking for skills - Mathematics\n",
    "    if(\"math\" in descr.lower()):\n",
    "        skills['Mathematics'].append('1')\n",
    "    else:\n",
    "        skills['Mathematics'].append('0')\n",
    "# looking for skills - Reinforcement Learning\n",
    "    if(\"reinforcement learning\" in descr.lower()):\n",
    "        skills['Reinforcement_Learning'].append('1')\n",
    "    else:\n",
    "        skills['Reinforcement_Learning'].append('0')\n",
    "# looking for skills - Deep Learning    \n",
    "    if(\"deep learning\" in descr.lower()):\n",
    "        skills['Deep_Learning'].append('1')\n",
    "    else:\n",
    "        skills['Deep_Learning'].append('0')\n",
    "# looking for skills - Machine Learning                                   \n",
    "    if(\"machine learning\" in descr.lower()):\n",
    "        skills['Machine_Learning'].append('1')\n",
    "    else:\n",
    "        skills['Machine_Learning'].append('0')\n",
    "# looking for skills - Data Visualization\n",
    "    if(\"visualization\" in descr.lower()):\n",
    "        skills['Data_Visualization'].append('1')\n",
    "    else:\n",
    "        skills['Data_Visualization'].append('0')\n",
    "# looking for skills - Keras\n",
    "    if(\"keras\" in descr.lower()):\n",
    "        skills['Keras'].append('1')\n",
    "    else:\n",
    "        skills['Keras'].append('0')\n",
    "# looking for skills - TensorFlow\n",
    "    if(\"tensorflow\" in descr.lower()):\n",
    "        skills['TensorFlow'].append('1')\n",
    "    else:\n",
    "        skills['TensorFlow'].append('0')\n",
    "# looking for skills - Artificial Intelligence\n",
    "    if(\"artificial intelligence\" in descr.lower()):\n",
    "        skills['Artificial_Intelligence'].append('1')\n",
    "    else:\n",
    "        skills['Artificial_Intelligence'].append('0')\n",
    "# looking for skills - Communications\n",
    "    if(\"communication\" in descr.lower()):\n",
    "        skills['Communications'].append('1')\n",
    "    else:\n",
    "        skills['Communications'].append('0')\n",
    "# looking for skills - Team Player\n",
    "    if(\"team work\" in descr.lower() or \"team spirit\" in descr.lower() or \"team member\" in descr.lower() \n",
    "       or \"team player\" in descr.lower()):\n",
    "        skills['Team_Player'].append('1')\n",
    "    else:\n",
    "        skills['Team_Player'].append('0')\n",
    "# looking for skills - Empathy\n",
    "    if(\"empathy\" in descr.lower()):\n",
    "        skills['Empathy'].append('1')\n",
    "    else:\n",
    "        skills['Empathy'].append('0')\n",
    "# looking for skills - Emotional Intellect\n",
    "    if(\"emotional intellect\" in descr.lower()):\n",
    "        skills['Emotional_Intellect'].append('1')\n",
    "    else:\n",
    "        skills['Emotional_Intellect'].append('0')\n",
    "# looking for skills - Self-Improvement\n",
    "    if(\"self-improvement\" in descr.lower() or \"personal development\" in descr.lower()):\n",
    "        skills['Self_Improvement'].append('1')\n",
    "    else:\n",
    "        skills['Self_Improvement'].append('0')\n",
    "# looking for skills - Presentation\n",
    "    if(\"presentation\" in descr.lower()):\n",
    "        skills['Presentation'].append('1')\n",
    "    else:\n",
    "        skills['Presentation'].append('0')\n",
    "# looking for skills - Critical Thinking\n",
    "    if(\"critical thinking\" in descr.lower()):\n",
    "        skills['Critical_Thinking'].append('1')\n",
    "    else:\n",
    "        skills['Critical_Thinking'].append('0')\n",
    "# looking for skills - Business Understanding\n",
    "    if(\"business understanding\" in descr.lower() or \"understand business\" in descr.lower()):\n",
    "        skills['Business_Understanding'].append('1')\n",
    "    else:\n",
    "        skills['Business_Understanding'].append('0')\n",
    "    \n",
    "    if other_comp_flag==False:\n",
    "        other_compensations.append(\"NA\")\n",
    "    \n",
    "#     try:\n",
    "#         content = Jb_soup.find(\"div\", class_=\"content\")\n",
    "#         refID = re.search(\"Ref ID: [0-9]+-+[0-9]+\", content.text)\n",
    "#         job_ids.append(refID.group()[8:])\n",
    "#     except:\n",
    "#         job_ids.append(\"NA\")    \n",
    "        \n",
    "#     try:\n",
    "#         job_descriptions.append(str(re.sub('(\\n|\\r)', '',content.text)))\n",
    "#     except:\n",
    "#         job_descriptions.append(\"NA\")\n",
    "        \n",
    "        \n",
    "# print(job_descriptions)\n",
    "\n",
    "#extract all jobsfor company with many vacancies\n",
    "for item in jb_all:\n",
    "    \n",
    "    # extract job_title\n",
    "    jobs = item.findAll(\"div\", class_=re.compile(\"searchresult clearfix\"))\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            titles.append(job[\"title\"])\n",
    "        except:\n",
    "            titles.append(\"NA\")\n",
    "            \n",
    "    # extract company_name\n",
    "    companies = item.findAll(\"span\", class_=\"resultcompany_mobile clearfix\" )\n",
    "    for company in companies:\n",
    "        try:\n",
    "            company_names.append(company.text.strip())\n",
    "        except:\n",
    "            company_names.append(\"NA\")\n",
    "\n",
    "    # extract location       \n",
    "    locations = item.findAll(\"span\", class_=\"result-right-one\")\n",
    "    for location in locations:\n",
    "        try:\n",
    "            location_names.append(location.text)\n",
    "        except:\n",
    "            location_names.append(\"NA\")\n",
    "\n",
    "    # extract compensation       \n",
    "    payments = item.findAll(\"span\", class_=\"result-right-two non-mobile\")\n",
    "    for payment in payments:\n",
    "        try:\n",
    "            compensation.append(payment.text)\n",
    "        except:\n",
    "            compensation.append(\"NA\")\n",
    "\n",
    "    #extract job description and all information rom it\n",
    "    links = item.findAll(\"a\", class_ = \"result-left\")\n",
    "    for link in links:\n",
    "        ind+=1\n",
    "        print(ind, link[\"href\"])\n",
    "        Jb = uReq(\"https://www.headhunter.com\" + link[\"href\"])\n",
    "        Jb_html = Jb.read()\n",
    "        Jb.close()\n",
    "        Jb_soup = soup(Jb_html, \"html.parser\")\n",
    "        \n",
    "        try:\n",
    "            refs.append(\"https://www.headhunter.com\" + link[\"href\"])\n",
    "        except:\n",
    "            refs.append(\"NA\")\n",
    "\n",
    "        job_text = Jb_soup.findAll(\"div\", id=(\"description-container-premium\", \"description-container-standard\"))\n",
    "        item = job_text[0]\n",
    "        contents = item.select(\"div.content\")\n",
    "\n",
    "        refID = re.search(\"Ref ID: [0-9]+-+[0-9]+\", contents[0].text)\n",
    "        try:\n",
    "            job_ids.append(refID.group()[8:])\n",
    "        except:\n",
    "            job_ids.append(\"NA\")\n",
    "\n",
    "        descr=''\n",
    "        other_comp_flag=False\n",
    "        for content in contents:\n",
    "        \n",
    "            descr+= content.text\n",
    "\n",
    "            for i in content.select(\"div#job-snapshot-container.content div.jobs-detail-summary-item span\"):\n",
    "        #         if i.text == 'Location ':\n",
    "        #             print(i.next_sibling.string.next_sibling.text)\n",
    "\n",
    "        #         other_compensations.append(\"NA\")\n",
    "\n",
    "                if i.text == 'Employment Type ':\n",
    "                    try:\n",
    "                        employment_types.append(i.next_sibling.string.next_sibling.text)\n",
    "                    except:\n",
    "                        employment_types.append(\"NA\")\n",
    "                if i.text == 'Pay Type ':\n",
    "                    try:\n",
    "                        pay_types.append(i.next_sibling.string.next_sibling.text)\n",
    "                    except:\n",
    "                        pay_types.append(\"NA\")\n",
    "                if i.text == 'Pay Rate ':\n",
    "                    try:\n",
    "                        pay_rates.append(i.next_sibling.string.next_sibling.text)\n",
    "                    except:\n",
    "                        pay_rates.append(\"NA\")\n",
    "                if i.text == 'Store Type ':\n",
    "                    try:\n",
    "                        store_types.append(i.next_sibling.string.next_sibling.text)\n",
    "                    except:\n",
    "                        store_types.append(\"NA\")\n",
    "                if i.text == 'Other Compensation: ':\n",
    "                    other_comp_flag=True\n",
    "                    try:\n",
    "                        other_compensations.append(i.next_sibling.string.next_sibling.text)\n",
    "                    except:\n",
    "                        other_compensations.append(\"NA\")\n",
    "\n",
    "        try:\n",
    "            job_descriptions.append(str(re.sub('(\\n|\\r)', '',descr)))\n",
    "        except:\n",
    "            job_descriptions.append(\"NA\")\n",
    "            \n",
    "    # looking for skills - Excell\n",
    "        if(\"excel\" in descr.lower()):\n",
    "            skills['Excel'].append('1')\n",
    "        else:\n",
    "            skills['Excel'].append('0')\n",
    "    # looking for skills - Python    \n",
    "        if(\"python\" in descr.lower()):\n",
    "            skills['Python'].append('1')\n",
    "        else:\n",
    "            skills['Python'].append('0')\n",
    "    # looking for skills - R\n",
    "        if(\"R \" in descr):\n",
    "            skills['R '].append('1')\n",
    "        else:\n",
    "            skills['R '].append('0')\n",
    "    # looking for skills - Matlab\n",
    "        if(\"matlab\" in descr.lower()):\n",
    "            skills['Matlab'].append('1')\n",
    "        else:\n",
    "            skills['Matlab'].append('0')\n",
    "    # looking for skills - SAS\n",
    "        if(\"sas\" in descr.lower()):\n",
    "            skills['SAS'].append('1')\n",
    "        else:\n",
    "            skills['SAS'].append('0')\n",
    "    # looking for skills - SQL\n",
    "        if(\"sql\" in descr.lower()):\n",
    "            skills['SQL'].append('1')\n",
    "        else:\n",
    "            skills['SQL'].append('0')\n",
    "    # looking for skills - SPSS\n",
    "        if(\"spss\" in descr.lower()):\n",
    "            skills['SPSS'].append('1')\n",
    "        else:\n",
    "            skills['SPSS'].append('0')\n",
    "    # looking for skills - Hadoop\n",
    "        if(\"hadoop\" in descr.lower()):\n",
    "            skills['Hadoop'].append('1')\n",
    "        else:\n",
    "            skills['Hadoop'].append('0')\n",
    "    # looking for skills - Spark\n",
    "        if(\"spark\" in descr.lower()):\n",
    "            skills['Spark'].append('1')\n",
    "        else:\n",
    "            skills['Spark'].append('0')\n",
    "    # looking for skills - Algebra\n",
    "        if(\"algebra\" in descr.lower()):\n",
    "            skills['Algebra'].append('1')\n",
    "        else:\n",
    "            skills['Algebra'].append('0')\n",
    "    # looking for skills - Statistic\n",
    "        if(\"statistic\" in descr.lower()):\n",
    "            skills['Statistic'].append('1')\n",
    "        else:\n",
    "            skills['Statistic'].append('0')\n",
    "    # looking for skills - Mathematics\n",
    "        if(\"math\" in descr.lower()):\n",
    "            skills['Mathematics'].append('1')\n",
    "        else:\n",
    "            skills['Mathematics'].append('0')\n",
    "    # looking for skills - Reinforcement Learning\n",
    "        if(\"reinforcement learning\" in descr.lower()):\n",
    "            skills['Reinforcement_Learning'].append('1')\n",
    "        else:\n",
    "            skills['Reinforcement_Learning'].append('0')\n",
    "    # looking for skills - Deep Learning    \n",
    "        if(\"deep learning\" in descr.lower()):\n",
    "            skills['Deep_Learning'].append('1')\n",
    "        else:\n",
    "            skills['Deep_Learning'].append('0')\n",
    "    # looking for skills - Machine Learning                                   \n",
    "        if(\"machine learning\" in descr.lower()):\n",
    "            skills['Machine_Learning'].append('1')\n",
    "        else:\n",
    "            skills['Machine_Learning'].append('0')\n",
    "    # looking for skills - Data Visualization\n",
    "        if(\"visualization\" in descr.lower()):\n",
    "            skills['Data_Visualization'].append('1')\n",
    "        else:\n",
    "            skills['Data_Visualization'].append('0')\n",
    "    # looking for skills - Keras\n",
    "        if(\"keras\" in descr.lower()):\n",
    "            skills['Keras'].append('1')\n",
    "        else:\n",
    "            skills['Keras'].append('0')\n",
    "    # looking for skills - TensorFlow\n",
    "        if(\"tensorflow\" in descr.lower()):\n",
    "            skills['TensorFlow'].append('1')\n",
    "        else:\n",
    "            skills['TensorFlow'].append('0')\n",
    "    # looking for skills - Artificial Intelligence\n",
    "        if(\"artificial intelligence\" in descr.lower()):\n",
    "            skills['Artificial_Intelligence'].append('1')\n",
    "        else:\n",
    "            skills['Artificial_Intelligence'].append('0')\n",
    "    # looking for skills - Communications\n",
    "        if(\"communication\" in descr.lower()):\n",
    "            skills['Communications'].append('1')\n",
    "        else:\n",
    "            skills['Communications'].append('0')\n",
    "    # looking for skills - Team Player\n",
    "        if(\"team work\" in descr.lower() or \"team spirit\" in descr.lower() or \"team member\" in descr.lower() \n",
    "           or \"team player\" in descr.lower()):\n",
    "            skills['Team_Player'].append('1')\n",
    "        else:\n",
    "            skills['Team_Player'].append('0')\n",
    "    # looking for skills - Empathy\n",
    "        if(\"empathy\" in descr.lower()):\n",
    "            skills['Empathy'].append('1')\n",
    "        else:\n",
    "            skills['Empathy'].append('0')\n",
    "    # looking for skills - Emotional Intellect\n",
    "        if(\"emotional intellect\" in descr.lower()):\n",
    "            skills['Emotional_Intellect'].append('1')\n",
    "        else:\n",
    "            skills['Emotional_Intellect'].append('0')\n",
    "    # looking for skills - Self-Improvement\n",
    "        if(\"self-improvement\" in descr.lower() or \"personal development\" in descr.lower()):\n",
    "            skills['Self_Improvement'].append('1')\n",
    "        else:\n",
    "            skills['Self_Improvement'].append('0')\n",
    "    # looking for skills - Presentation\n",
    "        if(\"presentation\" in descr.lower()):\n",
    "            skills['Presentation'].append('1')\n",
    "        else:\n",
    "            skills['Presentation'].append('0')\n",
    "    # looking for skills - Critical Thinking\n",
    "        if(\"critical thinking\" in descr.lower()):\n",
    "            skills['Critical_Thinking'].append('1')\n",
    "        else:\n",
    "            skills['Critical_Thinking'].append('0')\n",
    "    # looking for skills - Business Understanding\n",
    "        if(\"business understanding\" in descr.lower() or \"understand business\" in descr.lower()):\n",
    "            skills['Business_Understanding'].append('1')\n",
    "        else:\n",
    "            skills['Business_Understanding'].append('0')\n",
    "\n",
    "        if other_comp_flag==False:\n",
    "            other_compensations.append(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas dataframe from list and dictionaries filled during parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Job_Title':titles,'Company_Name':company_names, 'Location':location_names, 'Compensation': compensation, \n",
    "     'Job_ID':job_ids,'Job_Description':job_descriptions, 'Employment_Type':employment_types, 'Pay_Type':pay_types,\n",
    "     'Pay_Rate':pay_rates, 'Store_Type':store_types, 'Other_Compensation':other_compensations, 'Link': refs}\n",
    "\n",
    "df1 = pd.DataFrame(d)\n",
    "df2 = pd.DataFrame(skills)\n",
    "frames = [df1, df2]\n",
    "results = pd.concat(frames, axis = 1)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we parse data from short list and full list - there will be duplicates for sure. Hence removing duplicates using pandas dataframe method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))\n",
    "result_unique = results.drop_duplicates()\n",
    "print(len(result_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving dataframe into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_unique.to_csv('Headhunters_job_vacancies.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
